{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|S| = 45\n",
      "|O| = 35\n"
     ]
    }
   ],
   "source": [
    "from cardsample_problem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Testing POMCP ***\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(20000.000, 0.365 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(10187.000, 0.365 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(9814.000, 0.354 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.3649749755859375, Action(1): 0.3538820445537567}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(25116.000, -0.190 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(21557.000, -0.190 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(3560.000, -0.824 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): -0.190425306558609, Action(1): -0.8238768577575684}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(34480.000, -1.502 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(17438.000, -1.502 | dict_keys([Observation((1, 2, 0, 0)), Observation((2, 1, 0, 0))])), Action(1): QNode(17043.000, -1.508 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -1.501835823059082, Action(1): -1.5075957775115967}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28091.000, -2.604 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14048.000, -2.604 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14044.000, -2.604 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.604356527328491, Action(1): -2.6044561862945557}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12541.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 1\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(40000.000, 0.450 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(21137.000, 0.450 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(18864.000, 0.424 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.44992098212242126, Action(1): 0.4236108362674713}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(30521.000, -0.059 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(3921.000, -0.690 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(26601.000, -0.059 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.6903845071792603, Action(1): -0.05868155509233475}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(37843.000, -1.385 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(18834.000, -1.387 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(19010.000, -1.385 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -1.3870667219161987, Action(1): -1.3849520683288574}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(27951.000, -2.609 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(13974.000, -2.609 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(13978.000, -2.609 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.609417676925659, Action(1): -2.6093151569366455}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12498.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.5 *** iter = 2\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(60000.000, 0.483 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(31503.000, 0.483 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(28498.000, 0.464 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.4833194315433502, Action(1): 0.46378669142723083}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(35647.000, 0.052 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(4049.000, -0.602 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(31599.000, 0.052 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.6016295552253723, Action(1): 0.051710691303014755}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(41085.000, -1.268 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(20829.000, -1.268 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(20257.000, -1.274 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -1.2675137519836426, Action(1): -1.2738336324691772}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28280.000, -2.597 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14141.000, -2.597 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(14140.000, -2.597 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.597201108932495, Action(1): -2.5972416400909424}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12590.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.6666666666666665 *** iter = 3\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(80000.000, 0.513 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(42162.000, 0.513 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(37839.000, 0.495 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.5129493474960327, Action(1): 0.4948335886001587}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(41178.000, 0.127 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(36910.000, 0.127 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(4269.000, -0.531 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.12747181951999664, Action(1): -0.5310372114181519}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(32312.000, -0.443 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(31189.000, -0.443 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1124.000, -2.001 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.4433594048023224, Action(1): -2.000889301300049}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(45476.000, -2.309 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(22739.000, -2.309 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(22738.000, -2.309 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.309117555618286, Action(1): -2.309131383895874}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19772.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.25 *** iter = 4\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(100000.000, 0.534 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(52414.000, 0.534 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(47587.000, 0.519 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.5336916446685791, Action(1): 0.5192610621452332}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(46158.000, 0.195 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(4133.000, -0.505 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(42026.000, 0.195 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.5052023530006409, Action(1): 0.1946420818567276}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(48114.000, -1.085 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(24131.000, -1.085 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(23984.000, -1.086 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -1.0849127769470215, Action(1): -1.0861371755599976}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28125.000, -2.601 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14065.000, -2.601 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(14061.000, -2.601 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.601348638534546, Action(1): -2.6014490127563477}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12526.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.4 *** iter = 5\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(120000.000, 0.549 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(63451.000, 0.549 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(56550.000, 0.533 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.5487051010131836, Action(1): 0.5328601002693176}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(51753.000, 0.240 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(47248.000, 0.240 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(4506.000, -0.439 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.24022139608860016, Action(1): -0.43852654099464417}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(35589.000, -0.307 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(34474.000, -0.307 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1116.000, -1.898 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.3069409728050232, Action(1): -1.8978500366210938}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(45475.000, -2.310 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(22738.000, -2.310 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(22738.000, -2.310 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.310009479522705, Action(1): -2.310009479522705}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19791.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.1666666666666665 *** iter = 6\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(140000.000, 0.556 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(72512.000, 0.556 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(67489.000, 0.547 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.5559194087982178, Action(1): 0.5469653010368347}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(56312.000, 0.274 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(51431.000, 0.274 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(4882.000, -0.381 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.2740611433982849, Action(1): -0.381196528673172}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(37130.000, -0.256 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(36018.000, -0.256 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1113.000, -1.861 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.2563919723033905, Action(1): -1.8607375621795654}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(45655.000, -2.305 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(22830.000, -2.305 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(22826.000, -2.305 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.305213212966919, Action(1): -2.3052213191986084}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 1.0\n",
      "VNode(19801.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 7\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(160000.000, 0.566 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(83628.000, 0.566 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(76373.000, 0.555 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.5662081241607666, Action(1): 0.5550930500030518}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(61814.000, 0.315 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(4506.000, -0.398 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(57309.000, 0.315 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.39791393280029297, Action(1): 0.31486400961875916}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(39039.000, -0.196 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1108.000, -1.817 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(37932.000, -0.196 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.8167860507965088, Action(1): -0.19603516161441803}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(45750.000, -2.303 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(22875.000, -2.303 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(22876.000, -2.303 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.302950143814087, Action(1): -2.30293607711792}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19810.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 8\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(180000.000, 0.577 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(96726.000, 0.577 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(83275.000, 0.560 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.5770383477210999, Action(1): 0.5596417784690857}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(68297.000, 0.344 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(4611.000, -0.375 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(63687.000, 0.344 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.374972939491272, Action(1): 0.343536376953125}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(62578.000, -0.842 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(30349.000, -0.853 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(32230.000, -0.842 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.8530418276786804, Action(1): -0.8417304754257202}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28387.000, -2.590 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14192.000, -2.590 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14196.000, -2.590 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.589909315109253, Action(1): -2.5898125171661377}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12572.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 9\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(200000.000, 0.583 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(106733.000, 0.583 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(93268.000, 0.568 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.5828546285629272, Action(1): 0.5679877400398254}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(73305.000, 0.368 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(4805.000, -0.342 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(68501.000, 0.368 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.34235140681266785, Action(1): 0.36771512031555176}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(65805.000, -0.798 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(32111.000, -0.807 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(33695.000, -0.798 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.807014524936676, Action(1): -0.798190176486969}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(28353.000, -2.591 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14177.000, -2.591 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14177.000, -2.591 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.590531349182129, Action(1): -2.590531349182129}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12553.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 10\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(220000.000, 0.588 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(117983.000, 0.588 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(102018.000, 0.573 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.588405430316925, Action(1): 0.5729249119758606}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(78941.000, 0.392 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(4927.000, -0.319 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(74015.000, 0.392 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.31865254044532776, Action(1): 0.3918663263320923}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(69528.000, -0.757 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(33719.000, -0.768 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(35810.000, -0.757 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.7677279114723206, Action(1): -0.7569388747215271}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(28489.000, -2.587 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14245.000, -2.587 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14245.000, -2.587 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.586731195449829, Action(1): -2.586731195449829}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12607.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 11\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(240000.000, 0.592 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(127438.000, 0.592 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(112563.000, 0.579 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.5919886827468872, Action(1): 0.5791184902191162}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(83612.000, 0.404 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(4962.000, -0.312 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(78651.000, 0.404 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.3121727406978607, Action(1): 0.4041531980037689}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(46277.000, -0.011 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1093.000, -1.687 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(45185.000, -0.011 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.687100887298584, Action(1): -0.011467576958239079}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46023.000, -2.297 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23012.000, -2.297 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23012.000, -2.297 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.296539306640625, Action(1): -2.296539306640625}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19840.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 12\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(260000.000, 0.599 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(141594.000, 0.599 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(118407.000, 0.581 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.5985606908798218, Action(1): 0.580958366394043}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(90866.000, 0.424 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(85453.000, 0.424 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(5414.000, -0.264 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.4236493706703186, Action(1): -0.263760507106781}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(76912.000, -0.682 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(39567.000, -0.682 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(37346.000, -0.692 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.6822866201400757, Action(1): -0.6922029852867126}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28383.000, -2.590 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14192.000, -2.590 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14192.000, -2.590 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.5904033184051514, Action(1): -2.5904033184051514}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(12575.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9230769230769231 *** iter = 13\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(280000.000, 0.605 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(156779.000, 0.605 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(123222.000, 0.582 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6048699021339417, Action(1): 0.5818124413490295}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(98281.000, 0.449 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5069.000, -0.282 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(93213.000, 0.449 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.2823042571544647, Action(1): 0.4486146569252014}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(82337.000, -0.638 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(40007.000, -0.647 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(42331.000, -0.638 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.646960437297821, Action(1): -0.6375936269760132}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28239.000, -2.594 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14118.000, -2.594 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14122.000, -2.594 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.593920946121216, Action(1): -2.593824863433838}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 3.0\n",
      "VNode(12511.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 14\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(300000.000, 0.606 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(165578.000, 0.606 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(134423.000, 0.587 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.606366753578186, Action(1): 0.5872644782066345}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(102898.000, 0.450 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(97395.000, 0.450 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(5504.000, -0.249 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.4495891332626343, Action(1): -0.24854676425457}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(52609.000, 0.111 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(51526.000, 0.111 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1084.000, -1.602 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.1110469251871109, Action(1): -1.6023976802825928}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 2, 0, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 1, 0), $, 0), 4 1\n",
      "VNode(46084.000, -2.296 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23042.000, -2.296 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23043.000, -2.296 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 3, 1, 2, 1, 0), $, 0) Observation((1, 2, 0, 1))\n",
      "{Action(0): -2.2956767082214355, Action(1): -2.2956624031066895}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19860.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9333333333333333 *** iter = 15\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(320000.000, 0.608 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(174100.000, 0.608 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(145901.000, 0.592 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.608154296875, Action(1): 0.5922078490257263}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(106933.000, 0.464 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5210.000, -0.265 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(101724.000, 0.464 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.2654511332511902, Action(1): 0.46422716975212097}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(87947.000, -0.596 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(42954.000, -0.603 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(44994.000, -0.596 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.6030868887901306, Action(1): -0.5956130027770996}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 2, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 2, 2, 2, 2), $, 0), 4 1\n",
      "VNode(28600.000, -2.582 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14302.000, -2.582 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14299.000, -2.582 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 2, 2, 2, 2), $, 0) Observation((2, 0, 2, 0))\n",
      "{Action(0): -2.581667423248291, Action(1): -2.5817179679870605}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12624.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  2.0 *** iter = 16\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(340000.000, 0.610 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(184175.000, 0.610 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(155826.000, 0.595 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6095162034034729, Action(1): 0.5953563451766968}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(112277.000, 0.471 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(106674.000, 0.471 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(5604.000, -0.232 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.47060805559158325, Action(1): -0.23162031173706055}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(91126.000, -0.581 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(46619.000, -0.581 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(44508.000, -0.588 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.5807719230651855, Action(1): -0.5881185531616211}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28520.000, -2.585 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14262.000, -2.585 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14259.000, -2.585 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.5847015380859375, Action(1): -2.584751605987549}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(12604.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9411764705882353 *** iter = 17\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(360000.000, 0.612 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(193417.000, 0.612 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(166584.000, 0.599 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6117611527442932, Action(1): 0.5988705158233643}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(116864.000, 0.476 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(111189.000, 0.476 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(5676.000, -0.226 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.4755600690841675, Action(1): -0.2263919711112976}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(57338.000, 0.184 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(56259.000, 0.184 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1080.000, -1.553 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.18413060903549194, Action(1): -1.5527766942977905}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46156.000, -2.294 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23077.000, -2.294 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23080.000, -2.294 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.2941441535949707, Action(1): -2.2941510677337646}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19870.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8888888888888888 *** iter = 18\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(380000.000, 0.614 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(207947.000, 0.614 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(172054.000, 0.599 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6142153143882751, Action(1): 0.5986300110816956}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(124152.000, 0.487 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(118356.000, 0.487 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(5797.000, -0.213 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.4873061180114746, Action(1): -0.21338623762130737}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(98629.000, -0.531 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(50997.000, -0.531 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(47633.000, -0.541 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.5309520959854126, Action(1): -0.5413274765014648}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28462.000, -2.588 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14231.000, -2.588 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14232.000, -2.588 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.5880134105682373, Action(1): -2.5879709720611572}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12604.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.894736842105263 *** iter = 19\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(400000.000, 0.617 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(220862.000, 0.617 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(179139.000, 0.600 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6169416308403015, Action(1): 0.5999573469161987}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(130606.000, 0.496 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(124786.000, 0.496 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(5821.000, -0.210 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.496188759803772, Action(1): -0.20958614349365234}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(61841.000, 0.244 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(60765.000, 0.244 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1077.000, -1.512 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.24405939877033234, Action(1): -1.5116060972213745}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46171.000, -2.294 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23088.000, -2.294 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23084.000, -2.294 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.294135808944702, Action(1): -2.2941417694091797}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19882.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.85 *** iter = 20\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(420000.000, 0.618 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(230529.000, 0.618 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(189472.000, 0.602 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6178144812583923, Action(1): 0.6023419499397278}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(135501.000, 0.503 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(129642.000, 0.503 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(5860.000, -0.205 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.5025784373283386, Action(1): -0.20460765063762665}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(106171.000, -0.495 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(54930.000, -0.495 | dict_keys([Observation((1, 2, 0, 0)), Observation((2, 1, 0, 0))])), Action(1): QNode(51242.000, -0.506 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.4953937828540802, Action(1): -0.5056590437889099}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 2, 0, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 1, 0), $, 0), 4 1\n",
      "VNode(28549.000, -2.583 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14273.000, -2.584 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14277.000, -2.583 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 3, 1, 2, 1, 0), $, 0) Observation((1, 2, 0, 1))\n",
      "{Action(0): -2.5835487842559814, Action(1): -2.5834569931030273}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 1.0\n",
      "VNode(12610.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8095238095238095 *** iter = 21\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(440000.000, 0.618 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(234621.000, 0.618 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(205380.000, 0.608 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6182584166526794, Action(1): 0.6081025004386902}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(137064.000, 0.510 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5517.000, -0.227 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(131548.000, 0.510 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.2265724390745163, Action(1): 0.5100714564323425}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(108006.000, -0.487 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(53550.000, -0.490 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(54457.000, -0.487 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.4899536073207855, Action(1): -0.48748642206192017}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28466.000, -2.585 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14233.000, -2.585 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14234.000, -2.585 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.5845584869384766, Action(1): -2.5845158100128174}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12558.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8636363636363635 *** iter = 22\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(460000.000, 0.620 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(244226.000, 0.620 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(215775.000, 0.610 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6195544600486755, Action(1): 0.6102968454360962}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(141857.000, 0.514 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5511.000, -0.227 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(136347.000, 0.514 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.22736351191997528, Action(1): 0.5141898393630981}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(111146.000, -0.475 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(55073.000, -0.478 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(56074.000, -0.475 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.47770991921424866, Action(1): -0.4751395881175995}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(28306.000, -2.591 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14155.000, -2.591 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14152.000, -2.591 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.5909576416015625, Action(1): -2.5910122394561768}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12523.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8695652173913044 *** iter = 23\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(480000.000, 0.622 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(257912.000, 0.622 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(222089.000, 0.611 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6218617558479309, Action(1): 0.6107342839241028}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(148697.000, 0.521 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5630.000, -0.217 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(143068.000, 0.521 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.21651872992515564, Action(1): 0.5209255218505859}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(115599.000, -0.456 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(57167.000, -0.459 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(58433.000, -0.456 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.45872554183006287, Action(1): -0.45563238859176636}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(28312.000, -2.591 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14158.000, -2.591 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14155.000, -2.591 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.590620756149292, Action(1): -2.5906729698181152}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12523.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.875 *** iter = 24\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(500000.000, 0.623 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(266861.000, 0.623 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(233140.000, 0.613 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6226366758346558, Action(1): 0.6129647493362427}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(153172.000, 0.525 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5744.000, -0.207 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(147429.000, 0.525 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.20682452619075775, Action(1): 0.5253954529762268}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(68877.000, 0.322 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1075.000, -1.459 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(67803.000, 0.322 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.4586048126220703, Action(1): 0.3220970630645752}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(46175.000, -2.295 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23086.000, -2.295 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23090.000, -2.295 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.2945492267608643, Action(1): -2.294541597366333}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19894.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.88 *** iter = 25\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(520000.000, 0.625 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(280798.000, 0.625 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(239203.000, 0.613 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6248089075088501, Action(1): 0.6134382486343384}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(160605.000, 0.528 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(154469.000, 0.528 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6137.000, -0.180 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.5283045768737793, Action(1): -0.17956651747226715}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(122807.000, -0.427 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(62937.000, -0.427 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(59871.000, -0.434 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.42690330743789673, Action(1): -0.43379947543144226}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28491.000, -2.586 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14246.000, -2.586 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14246.000, -2.586 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.585566997528076, Action(1): -2.585566997528076}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12592.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8846153846153846 *** iter = 26\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(540000.000, 0.625 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(289479.000, 0.625 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(250522.000, 0.615 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6254897713661194, Action(1): 0.615220308303833}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(164445.000, 0.534 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5778.000, -0.205 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(158668.000, 0.534 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.20456910133361816, Action(1): 0.5336050987243652}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(72629.000, 0.358 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1074.000, -1.434 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(71556.000, 0.358 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.4338916540145874, Action(1): 0.35775503516197205}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46156.000, -2.295 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23077.000, -2.295 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23080.000, -2.295 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.2953155040740967, Action(1): -2.2953193187713623}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19897.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8888888888888888 *** iter = 27\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(560000.000, 0.628 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(304143.000, 0.628 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(255858.000, 0.616 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6277416944503784, Action(1): 0.6158607006072998}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(171758.000, 0.541 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5779.000, -0.202 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(165980.000, 0.541 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.20245704054832458, Action(1): 0.5406979918479919}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(75102.000, 0.378 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1076.000, -1.421 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(74027.000, 0.378 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.4210028648376465, Action(1): 0.37790411710739136}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46245.000, -2.293 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23123.000, -2.293 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23123.000, -2.293 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.292954206466675, Action(1): -2.292954206466675}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 2.0\n",
      "VNode(19901.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8928571428571428 *** iter = 28\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(580000.000, 0.629 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(316177.000, 0.629 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(263824.000, 0.616 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6287639141082764, Action(1): 0.6164358258247375}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(177762.000, 0.548 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5810.000, -0.197 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(171953.000, 0.548 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.19672974944114685, Action(1): 0.5479221940040588}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(134976.000, -0.389 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(66805.000, -0.391 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(68172.000, -0.389 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.39122724533081055, Action(1): -0.3885611295700073}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28587.000, -2.579 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14292.000, -2.579 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14296.000, -2.579 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.5790655612945557, Action(1): -2.578974485397339}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12576.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9310344827586208 *** iter = 29\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(600000.000, 0.629 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(325626.000, 0.629 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(274375.000, 0.618 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6292628049850464, Action(1): 0.6181001663208008}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(182471.000, 0.548 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5837.000, -0.198 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(176635.000, 0.548 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.19804702699184418, Action(1): 0.5479952096939087}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(78769.000, 0.406 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1076.000, -1.401 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(77694.000, 0.406 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.401485800743103, Action(1): 0.40579143166542053}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46350.000, -2.290 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23176.000, -2.290 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23175.000, -2.290 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.2902159690856934, Action(1): -2.2902283668518066}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19906.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9333333333333333 *** iter = 30\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(620000.000, 0.629 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(330227.000, 0.629 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(289774.000, 0.621 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6292546391487122, Action(1): 0.6206600666046143}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(184734.000, 0.551 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(5869.000, -0.194 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(178866.000, 0.551 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.19355933368206024, Action(1): 0.5509023070335388}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(79286.000, 0.412 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1075.000, -1.396 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(78212.000, 0.412 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.396278738975525, Action(1): 0.4123973250389099}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46139.000, -2.296 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23072.000, -2.296 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23068.000, -2.296 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.296159267425537, Action(1): -2.2961671352386475}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19908.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.935483870967742 *** iter = 31\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(640000.000, 0.630 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(341027.000, 0.630 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(298974.000, 0.621 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6299712061882019, Action(1): 0.6214625239372253}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(190891.000, 0.551 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(184545.000, 0.551 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6347.000, -0.162 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.5514716506004333, Action(1): -0.1618087887763977}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(142905.000, -0.366 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(73803.000, -0.366 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(69103.000, -0.374 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.3657843768596649, Action(1): -0.3742966055870056}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28441.000, -2.586 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14219.000, -2.586 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14223.000, -2.586 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.5856239795684814, Action(1): -2.585529088973999}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12554.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9375 *** iter = 32\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(660000.000, 0.630 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(345969.000, 0.630 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(314032.000, 0.624 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6299751400947571, Action(1): 0.6239756941795349}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(193375.000, 0.551 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(187003.000, 0.551 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6373.000, -0.162 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.5511613488197327, Action(1): -0.16161943972110748}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(144376.000, -0.362 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(74746.000, -0.362 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(69631.000, -0.371 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.3619726598262787, Action(1): -0.37107065320014954}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 2, 0, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 1, 0), $, 0), 4 1\n",
      "VNode(28520.000, -2.584 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14259.000, -2.584 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14262.000, -2.584 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 3, 1, 2, 1, 0), $, 0) Observation((1, 2, 0, 1))\n",
      "{Action(0): -2.584470510482788, Action(1): -2.584418773651123}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(12600.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9090909090909092 *** iter = 33\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(680000.000, 0.630 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(353344.000, 0.630 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(326657.000, 0.625 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6303521990776062, Action(1): 0.6254174709320068}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(197123.000, 0.554 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(190736.000, 0.554 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6388.000, -0.160 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.554246187210083, Action(1): -0.15967446565628052}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(147034.000, -0.356 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(76133.000, -0.356 | dict_keys([Observation((1, 2, 0, 0)), Observation((2, 1, 0, 0))])), Action(1): QNode(70902.000, -0.365 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.3563103675842285, Action(1): -0.3653919994831085}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28529.000, -2.583 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14265.000, -2.583 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14265.000, -2.583 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.583315134048462, Action(1): -2.583315134048462}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(12590.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8823529411764706 *** iter = 34\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(700000.000, 0.631 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(365917.000, 0.631 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(334084.000, 0.626 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6313254237174988, Action(1): 0.6256537437438965}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(202605.000, 0.559 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6040.000, -0.183 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(196566.000, 0.559 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.18294692039489746, Action(1): 0.559053361415863}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(151265.000, -0.348 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(74372.000, -0.352 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(76894.000, -0.348 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.35240331292152405, Action(1): -0.34822025895118713}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 2, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 2, 2, 2, 2), $, 0), 4 1\n",
      "VNode(28597.000, -2.583 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14301.000, -2.583 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14297.000, -2.583 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 2, 2, 2, 2), $, 0) Observation((2, 0, 2, 0))\n",
      "{Action(0): -2.5831072330474854, Action(1): -2.583197832107544}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 3.0\n",
      "VNode(12643.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9142857142857144 *** iter = 35\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(720000.000, 0.632 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(374344.000, 0.632 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(345657.000, 0.628 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6324333548545837, Action(1): 0.6277921795845032}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(207535.000, 0.560 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(201149.000, 0.560 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6387.000, -0.160 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.5597035884857178, Action(1): -0.16016919910907745}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(87103.000, 0.465 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(86027.000, 0.465 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1077.000, -1.361 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.4645886719226837, Action(1): -1.361188530921936}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46224.000, -2.294 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23113.000, -2.294 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23112.000, -2.294 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.2941195964813232, Action(1): -2.2941319942474365}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19915.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8888888888888888 *** iter = 36\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(740000.000, 0.633 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(385298.000, 0.633 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(354703.000, 0.628 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6333229541778564, Action(1): 0.6282703876495361}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(212317.000, 0.565 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6049.000, -0.182 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(206269.000, 0.565 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.18151767551898956, Action(1): 0.5649229288101196}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(88571.000, 0.472 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1079.000, -1.357 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(87493.000, 0.472 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.3568122386932373, Action(1): 0.4722427725791931}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46337.000, -2.291 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23171.000, -2.291 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23167.000, -2.291 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.2910096645355225, Action(1): -2.291017770767212}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19917.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8918918918918919 *** iter = 37\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(760000.000, 0.634 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(393921.000, 0.634 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(366080.000, 0.629 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.633708655834198, Action(1): 0.6294468641281128}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(216533.000, 0.567 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6143.000, -0.175 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(210391.000, 0.567 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.1751587837934494, Action(1): 0.5667251944541931}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(89878.000, 0.481 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1078.000, -1.350 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(88801.000, 0.481 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.3497217893600464, Action(1): 0.48089152574539185}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46256.000, -2.293 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23127.000, -2.293 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23130.000, -2.293 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.29333758354187, Action(1): -2.293341636657715}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19916.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.894736842105263 *** iter = 38\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(780000.000, 0.634 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(401235.000, 0.634 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(378766.000, 0.631 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6341728568077087, Action(1): 0.6308340430259705}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(220167.000, 0.569 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6131.000, -0.175 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(214037.000, 0.569 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.1753385215997696, Action(1): 0.5692163109779358}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(91068.000, 0.488 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1079.000, -1.347 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(89990.000, 0.488 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.346617341041565, Action(1): 0.48788225650787354}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(46243.000, -2.294 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23120.000, -2.294 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23124.000, -2.294 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.2937281131744385, Action(1): -2.293719530105591}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19919.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8974358974358974 *** iter = 39\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(800000.000, 0.635 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(413430.000, 0.635 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(386571.000, 0.631 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6348288655281067, Action(1): 0.6310409307479858}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(227196.000, 0.569 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(220757.000, 0.569 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6440.000, -0.157 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.569166362285614, Action(1): -0.15683236718177795}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(167123.000, -0.315 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(85168.000, -0.315 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(81956.000, -0.319 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.3146130442619324, Action(1): -0.31923291087150574}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28411.000, -2.586 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14206.000, -2.586 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14206.000, -2.586 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.58573579788208, Action(1): -2.58573579788208}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(12531.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.875 *** iter = 40\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(820000.000, 0.635 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(423497.000, 0.635 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(396504.000, 0.631 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6351005434989929, Action(1): 0.6312373876571655}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(231206.000, 0.573 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6158.000, -0.175 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(225049.000, 0.573 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.17489434778690338, Action(1): 0.5727453827857971}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(170346.000, -0.310 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(84414.000, -0.312 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(85933.000, -0.310 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.3118201196193695, Action(1): -0.30970701575279236}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28440.000, -2.586 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14220.000, -2.586 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14221.000, -2.586 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.5860745906829834, Action(1): -2.5860328674316406}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12559.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9024390243902438 *** iter = 41\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(840000.000, 0.636 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(433584.000, 0.636 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(406417.000, 0.632 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6359907388687134, Action(1): 0.6321566104888916}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(237313.000, 0.574 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(230836.000, 0.574 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6478.000, -0.154 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.5741216540336609, Action(1): -0.15375111997127533}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(174024.000, -0.304 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(88586.000, -0.304 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(85439.000, -0.308 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.30404359102249146, Action(1): -0.30833572149276733}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28463.000, -2.583 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14234.000, -2.583 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14230.000, -2.583 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.5831785202026367, Action(1): -2.5832736492156982}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 1.0\n",
      "VNode(12538.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.880952380952381 *** iter = 42\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(860000.000, 0.637 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(445035.000, 0.637 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(414966.000, 0.633 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6369137763977051, Action(1): 0.6327616572380066}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(242984.000, 0.575 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(236495.000, 0.575 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6490.000, -0.154 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.5754774212837219, Action(1): -0.15408341586589813}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(98811.000, 0.527 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(97730.000, 0.527 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1082.000, -1.319 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.5274338126182556, Action(1): -1.318853497505188}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46327.000, -2.292 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23164.000, -2.292 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23164.000, -2.292 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.2916600704193115, Action(1): -2.2916600704193115}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19924.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8604651162790697 *** iter = 43\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(880000.000, 0.638 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(457947.000, 0.638 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(422054.000, 0.634 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6379770040512085, Action(1): 0.6335659027099609}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(248519.000, 0.579 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6216.000, -0.172 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(242304.000, 0.579 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.1719754934310913, Action(1): 0.5792476534843445}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(181731.000, -0.289 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(89719.000, -0.292 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(92013.000, -0.289 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.2916322350502014, Action(1): -0.2887421250343323}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28336.000, -2.589 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14167.000, -2.589 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14170.000, -2.589 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.5888335704803467, Action(1): -2.588778018951416}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12516.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8636363636363635 *** iter = 44\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(900000.000, 0.638 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(469115.000, 0.638 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(430886.000, 0.634 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6384391784667969, Action(1): 0.6337400674819946}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(254084.000, 0.582 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6238.000, -0.170 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(247847.000, 0.582 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.1700865924358368, Action(1): 0.5817873477935791}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(185566.000, -0.284 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(91542.000, -0.287 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(94025.000, -0.284 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.2872016429901123, Action(1): -0.2841370105743408}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28374.000, -2.587 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14186.000, -2.587 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14189.000, -2.587 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.5873372554779053, Action(1): -2.587285280227661}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12525.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8666666666666667 *** iter = 45\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(920000.000, 0.639 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(481790.000, 0.639 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(438211.000, 0.634 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6392412781715393, Action(1): 0.6339724063873291}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(260435.000, 0.583 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6257.000, -0.170 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(254179.000, 0.583 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.17004945874214172, Action(1): 0.5831478834152222}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(189559.000, -0.278 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(93937.000, -0.280 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(95623.000, -0.278 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.2796770930290222, Action(1): -0.2776634693145752}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(28347.000, -2.590 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14174.000, -2.590 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14174.000, -2.590 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.5900959968566895, Action(1): -2.5900959968566895}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12542.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8695652173913044 *** iter = 46\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(940000.000, 0.640 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(492766.000, 0.640 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(447235.000, 0.635 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6399341225624084, Action(1): 0.6347566246986389}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(266801.000, 0.585 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(260187.000, 0.585 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6615.000, -0.146 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.5849154591560364, Action(1): -0.14588086307048798}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(106702.000, 0.563 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(105618.000, 0.563 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1085.000, -1.294 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.5625942945480347, Action(1): -1.2940094470977783}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 2, 0, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 1, 0), $, 0), 4 1\n",
      "VNode(46329.000, -2.292 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23163.000, -2.292 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23167.000, -2.292 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 3, 1, 2, 1, 0), $, 0) Observation((1, 2, 0, 1))\n",
      "{Action(0): -2.291844367980957, Action(1): -2.2918388843536377}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19931.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.851063829787234 *** iter = 47\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(960000.000, 0.641 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(503686.000, 0.641 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(456315.000, 0.635 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6405487656593323, Action(1): 0.6352684497833252}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(272252.000, 0.587 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(265614.000, 0.587 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6639.000, -0.144 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.5872023105621338, Action(1): -0.1441483199596405}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(108620.000, 0.569 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(107534.000, 0.569 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1087.000, -1.290 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.5692867636680603, Action(1): -1.2897894382476807}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46445.000, -2.289 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23225.000, -2.289 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23221.000, -2.289 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.2886555194854736, Action(1): -2.288663148880005}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19932.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8333333333333333 *** iter = 48\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(980000.000, 0.641 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(511942.000, 0.641 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(468059.000, 0.636 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.640568196773529, Action(1): 0.6358488202095032}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(276461.000, 0.589 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(269767.000, 0.589 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6695.000, -0.141 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.5885221362113953, Action(1): -0.14055274426937103}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(199812.000, -0.262 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(102036.000, -0.262 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(97777.000, -0.267 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.262093722820282, Action(1): -0.26682114601135254}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28184.000, -2.594 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14091.000, -2.594 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14094.000, -2.594 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.5942089557647705, Action(1): -2.5941519737243652}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 2.0\n",
      "VNode(12471.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.836734693877551 *** iter = 49\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(1000000.000, 0.641 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(520881.000, 0.641 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(479120.000, 0.637 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6410312652587891, Action(1): 0.6366249918937683}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(279948.000, 0.589 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6462.000, -0.157 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(273487.000, 0.589 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.1569172888994217, Action(1): 0.589247465133667}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(202294.000, -0.258 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(99857.000, -0.261 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(102438.000, -0.258 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -0.2612733542919159, Action(1): -0.2584686577320099}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(28258.000, -2.593 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14131.000, -2.593 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(14128.000, -2.593 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.593022108078003, Action(1): -2.5930774211883545}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12514.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.84 *** iter = 50\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(1020000.000, 0.641 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(531206.000, 0.641 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(488795.000, 0.637 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6411402821540833, Action(1): 0.6368823647499084}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(285078.000, 0.590 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6482.000, -0.156 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(278597.000, 0.590 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.15627886354923248, Action(1): 0.5901913046836853}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(205715.000, -0.255 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(101417.000, -0.258 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(104299.000, -0.255 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.2584186792373657, Action(1): -0.2553621828556061}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(28531.000, -2.583 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14264.000, -2.583 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14268.000, -2.583 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.58300518989563, Action(1): -2.5829122066497803}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12588.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8627450980392157 *** iter = 51\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(1040000.000, 0.641 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(539087.000, 0.641 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(500914.000, 0.638 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6414872407913208, Action(1): 0.6376193761825562}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(289028.000, 0.592 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6456.000, -0.158 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(282573.000, 0.592 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.15752781927585602, Action(1): 0.5919952392578125}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(114106.000, 0.591 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1089.000, -1.275 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(113018.000, 0.591 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.275481939315796, Action(1): 0.5909655094146729}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46350.000, -2.291 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23176.000, -2.291 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23175.000, -2.291 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.2914209365844727, Action(1): -2.2914347648620605}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19934.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8653846153846154 *** iter = 52\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(1060000.000, 0.642 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(551917.000, 0.642 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(508084.000, 0.638 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.641904890537262, Action(1): 0.6377913355827332}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(296451.000, 0.594 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(289544.000, 0.594 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6908.000, -0.129 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.5939244627952576, Action(1): -0.12854675948619843}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(116446.000, 0.600 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(115357.000, 0.600 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1090.000, -1.271 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.5997763872146606, Action(1): -1.2706414461135864}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46288.000, -2.293 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23145.000, -2.293 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23144.000, -2.293 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.2931926250457764, Action(1): -2.2932076454162598}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19935.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.849056603773585 *** iter = 53\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(1080000.000, 0.642 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(563607.000, 0.642 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(516394.000, 0.638 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6423920392990112, Action(1): 0.6380278468132019}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(302323.000, 0.595 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(295375.000, 0.595 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(6949.000, -0.126 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.5954726338386536, Action(1): -0.12634922564029694}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(118539.000, 0.606 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(117449.000, 0.606 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1091.000, -1.265 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.6058449745178223, Action(1): -1.2648943662643433}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46409.000, -2.290 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23205.000, -2.290 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23205.000, -2.290 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.2898948192596436, Action(1): -2.2898948192596436}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19936.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8333333333333333 *** iter = 54\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(1100000.000, 0.643 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(575521.000, 0.643 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(524480.000, 0.638 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.642918050289154, Action(1): 0.6382704973220825}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(307279.000, 0.596 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6600.000, -0.150 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(300680.000, 0.596 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.14954538643360138, Action(1): 0.5963587760925293}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "VNode(220335.000, -0.238 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(108863.000, -0.240 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(111473.000, -0.238 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 1, 1, 2, 2, 2), 0, 1) Observation((1, 0, 2, 0))\n",
      "{Action(0): -0.24007262289524078, Action(1): -0.23759156465530396}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 2), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 2, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 2, 2, 2, 2), $, 0), 4 1\n",
      "VNode(28496.000, -2.582 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14250.000, -2.582 | dict_keys([Observation((2, 0, 2, 0))])), Action(1): QNode(14247.000, -2.582 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 2, 2, 2, 2), $, 0) Observation((2, 0, 2, 0))\n",
      "{Action(0): -2.5816147327423096, Action(1): -2.5816650390625}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12541.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8545454545454545 *** iter = 55\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(1120000.000, 0.643 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(588660.000, 0.643 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(531341.000, 0.638 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6433895826339722, Action(1): 0.638359010219574}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(313885.000, 0.598 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6656.000, -0.146 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(307230.000, 0.598 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.1461838036775589, Action(1): 0.5977550148963928}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(122448.000, 0.618 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1093.000, -1.256 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(121356.000, 0.618 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.256174921989441, Action(1): 0.6183395981788635}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(46428.000, -2.289 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23216.000, -2.289 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23213.000, -2.289 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.289457321166992, Action(1): -2.2894511222839355}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 2.0\n",
      "VNode(19937.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8571428571428572 *** iter = 56\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(1140000.000, 0.644 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(598878.000, 0.644 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(541123.000, 0.639 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6439131498336792, Action(1): 0.6389024257659912}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(319053.000, 0.599 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6703.000, -0.144 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(312351.000, 0.599 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.14351771771907806, Action(1): 0.5991872549057007}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(124193.000, 0.623 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1095.000, -1.254 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(123099.000, 0.623 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.2538807392120361, Action(1): 0.6232322454452515}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46490.000, -2.288 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23246.000, -2.288 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23245.000, -2.288 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.2877514362335205, Action(1): -2.2877628803253174}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19939.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8596491228070176 *** iter = 57\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(1160000.000, 0.644 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(611204.000, 0.644 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(548797.000, 0.639 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6444078087806702, Action(1): 0.6391009092330933}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(325222.000, 0.599 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6749.000, -0.142 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(318474.000, 0.599 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.14194685220718384, Action(1): 0.5991799831390381}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(126316.000, 0.629 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1096.000, -1.250 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(125221.000, 0.629 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.2499996423721313, Action(1): 0.6288548707962036}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(46583.000, -2.285 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23290.000, -2.285 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23294.000, -2.285 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.285231351852417, Action(1): -2.2852234840393066}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19941.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8620689655172413 *** iter = 58\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(1180000.000, 0.645 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(619509.000, 0.645 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(560492.000, 0.640 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6445074677467346, Action(1): 0.6395347118377686}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(329340.000, 0.602 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6709.000, -0.143 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(322632.000, 0.602 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.1432403177022934, Action(1): 0.6016386151313782}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(235128.000, -0.224 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(117732.000, -0.224 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(117397.000, -0.224 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -0.2237710803747177, Action(1): -0.22407735884189606}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28385.000, -2.586 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14193.000, -2.586 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(14193.000, -2.586 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.5861330032348633, Action(1): -2.5861330032348633}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12516.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.88135593220339 *** iter = 59\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(1200000.000, 0.645 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(633733.000, 0.645 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(566268.000, 0.640 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6449786424636841, Action(1): 0.639593780040741}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(337270.000, 0.602 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(330222.000, 0.602 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(7049.000, -0.124 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.6018832921981812, Action(1): -0.12427312880754471}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(130292.000, 0.641 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(129196.000, 0.641 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1097.000, -1.241 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.6412827968597412, Action(1): -1.2406564950942993}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(46448.000, -2.289 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23225.000, -2.289 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23224.000, -2.289 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.289043664932251, Action(1): -2.289055109024048}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19942.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8666666666666667 *** iter = 60\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(1220000.000, 0.645 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(637945.000, 0.645 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(582056.000, 0.640 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.644890546798706, Action(1): 0.6404758095741272}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(338558.000, 0.603 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6730.000, -0.143 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(331829.000, 0.603 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.14279332756996155, Action(1): 0.603358268737793}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "VNode(241269.000, -0.217 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(121532.000, -0.217 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(119738.000, -0.219 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 1, 2, 1, 1), 0, 0) Observation((1, 1, 1, 0))\n",
      "{Action(0): -0.21709482371807098, Action(1): -0.2186189740896225}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(28355.000, -2.589 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14176.000, -2.590 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(14180.000, -2.589 | dict_keys([Observation((1, 1, 1, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.5895183086395264, Action(1): -2.5894241333007812}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12541.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8688524590163935 *** iter = 61\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(1240000.000, 0.645 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(648113.000, 0.645 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(591888.000, 0.641 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6450689435005188, Action(1): 0.6407996416091919}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(344455.000, 0.603 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(337409.000, 0.603 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(7047.000, -0.125 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.6028727293014526, Action(1): -0.1251598596572876}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(244775.000, -0.211 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(126862.000, -0.211 | dict_keys([Observation((1, 2, 0, 0)), Observation((2, 1, 0, 0))])), Action(1): QNode(117914.000, -0.219 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.21137958765029907, Action(1): -0.21876953542232513}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28535.000, -2.581 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14266.000, -2.581 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14270.000, -2.581 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.5811715126037598, Action(1): -2.5810787677764893}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12565.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8709677419354838 *** iter = 62\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(1260000.000, 0.645 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(659601.000, 0.645 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(600400.000, 0.641 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6454755067825317, Action(1): 0.6409767270088196}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(350133.000, 0.605 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(343045.000, 0.605 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(7089.000, -0.122 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.6050882935523987, Action(1): -0.12187916785478592}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(248767.000, -0.209 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(129127.000, -0.209 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(119641.000, -0.217 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.20937517285346985, Action(1): -0.21702440083026886}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28542.000, -2.580 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14272.000, -2.580 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14271.000, -2.580 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.5798795223236084, Action(1): -2.5799169540405273}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 2.0\n",
      "VNode(12552.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.873015873015873 *** iter = 63\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(1280000.000, 0.646 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(666408.000, 0.646 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(613593.000, 0.642 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6455840468406677, Action(1): 0.6417322158813477}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(352885.000, 0.605 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6771.000, -0.142 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(346115.000, 0.605 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.1423717588186264, Action(1): 0.6048336625099182}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(250816.000, -0.211 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(126529.000, -0.211 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(124288.000, -0.212 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -0.21065525710582733, Action(1): -0.2124178111553192}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28244.000, -2.592 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14124.000, -2.592 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(14121.000, -2.592 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.591618061065674, Action(1): -2.591669797897339}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12483.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.890625 *** iter = 64\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "VNode(1300000.000, 0.646 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(677612.000, 0.646 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(622389.000, 0.642 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 1, 1, 0, 0), 1, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6459344029426575, Action(1): 0.6419461965560913}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 1, 1, 0, 0), 1, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(358447.000, 0.606 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6909.000, -0.134 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(351539.000, 0.606 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.13388320803642273, Action(1): 0.6062530875205994}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(254428.000, -0.206 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(128376.000, -0.206 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(126053.000, -0.208 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -0.20628522336483002, Action(1): -0.20807911455631256}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(28418.000, -2.587 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14211.000, -2.587 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(14208.000, -2.587 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.5866565704345703, Action(1): -2.586709976196289}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(12551.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9076923076923078 *** iter = 65\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 0, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "VNode(1320000.000, 0.646 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(688268.000, 0.646 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(631733.000, 0.642 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((2, 1, 1, 0, 0, 0), 0, 1) Observation((1, 0, 0, 0))\n",
      "{Action(0): 0.6464555263519287, Action(1): 0.6424031853675842}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 1, 1, 0, 0, 0), 0, 1), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 0, 1)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "VNode(363842.000, 0.607 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(6906.000, -0.134 | dict_keys([Observation((1, 1, 0, 0)), Observation((2, 0, 0, 0))])), Action(1): QNode(356937.000, 0.607 | dict_keys([Observation((1, 0, 0, 1)), Observation((1, 0, 1, 0))]))} State((2, 1, 1, 1, 1, 0), 1, 0) Observation((1, 0, 0, 1))\n",
      "{Action(0): -0.13437587022781372, Action(1): 0.6073349118232727}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "VNode(138863.000, 0.664 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(1101.000, -1.226 | dict_keys([Observation((1, 1, 0, 1))])), Action(1): QNode(137763.000, 0.664 | dict_keys([Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 1) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.2261576652526855, Action(1): 0.6642166376113892}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(46354.000, -2.292 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23178.000, -2.292 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(23177.000, -2.292 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.291785717010498, Action(1): -2.2917959690093994}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(19945.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9090909090909092 *** iter = 66\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(1340000.000, 0.647 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(699387.000, 0.647 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(640614.000, 0.643 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.646816074848175, Action(1): 0.6427546739578247}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(369975.000, 0.608 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(362906.000, 0.608 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(7070.000, -0.125 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.6079782247543335, Action(1): -0.12503555417060852}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(261927.000, -0.199 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(135007.000, -0.199 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(126921.000, -0.205 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.19927825033664703, Action(1): -0.20531636476516724}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28559.000, -2.579 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14280.000, -2.579 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14280.000, -2.579 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.5794832706451416, Action(1): -2.5794832706451416}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(12559.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.9104477611940298 *** iter = 67\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "VNode(1360000.000, 0.647 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(709972.000, 0.647 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(650029.000, 0.643 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((0, 1, 0, 2, 0, 0), 1, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6468586921691895, Action(1): 0.6428316235542297}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 1, 0, 2, 0, 0), 1, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 2, 0, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "VNode(375272.000, 0.608 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(368197.000, 0.608 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(7076.000, -0.126 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 0, 2, 0, 0), 0, 0) Observation((0, 2, 0, 0))\n",
      "{Action(0): 0.6082140207290649, Action(1): -0.1256362348794937}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 0, 2, 0, 0), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "VNode(142856.000, 0.673 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(141754.000, 0.673 | dict_keys([Observation((1, 2, 0, 0))])), Action(1): QNode(1103.000, -1.219 | dict_keys([Observation((0, 2, 0, 1))]))} State((2, 3, 1, 2, 0, 0), 0, 1) Observation((1, 2, 0, 0))\n",
      "{Action(0): 0.6734070777893066, Action(1): -1.219401240348816}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 2, 0, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 1, 0), $, 0), 4 1\n",
      "VNode(46388.000, -2.291 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(23196.000, -2.291 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(23193.000, -2.291 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 3, 1, 2, 1, 0), $, 0) Observation((1, 2, 0, 1))\n",
      "{Action(0): -2.29091215133667, Action(1): -2.2909064292907715}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(19945.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8970588235294117 *** iter = 68\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(1380000.000, 0.647 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(716864.000, 0.647 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(663137.000, 0.643 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6469482183456421, Action(1): 0.6434856057167053}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "VNode(378753.000, 0.608 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(371624.000, 0.608 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(7130.000, -0.123 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((1, 2, 1, 2, 0, 0), 1, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.608493983745575, Action(1): -0.12286137789487839}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 2, 1, 2, 0, 0), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 2, 0, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "VNode(267660.000, -0.195 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(138083.000, -0.195 | dict_keys([Observation((1, 2, 0, 0)), Observation((2, 1, 0, 0))])), Action(1): QNode(129578.000, -0.202 | dict_keys([Observation((1, 1, 0, 1)), Observation((1, 1, 1, 0))]))} State((2, 3, 1, 2, 0, 0), 0, 0) Observation((1, 2, 0, 0))\n",
      "{Action(0): -0.19539698958396912, Action(1): -0.20156200230121613}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 1, 2, 0, 0), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28567.000, -2.580 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14284.000, -2.580 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14284.000, -2.580 | dict_keys([Observation((1, 2, 0, 1))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.5799503326416016, Action(1): -2.5799503326416016}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(12572.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8840579710144927 *** iter = 69\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 0, 0)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "VNode(1400000.000, 0.647 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(725498.000, 0.647 | dict_keys([Observation((1, 0, 0, 0)), Observation((0, 1, 0, 0))])), Action(1): QNode(674503.000, 0.644 | dict_keys([Observation((0, 0, 0, 1)), Observation((0, 0, 1, 0))]))} State((1, 1, 0, 1, 0, 0), 0, 0) Observation((0, 1, 0, 0))\n",
      "{Action(0): 0.6471150517463684, Action(1): 0.6439850926399231}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 1, 0, 1, 0, 0), 0, 0), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 0, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "VNode(383116.000, 0.609 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(375956.000, 0.609 | dict_keys([Observation((0, 2, 0, 0)), Observation((1, 1, 0, 0))])), Action(1): QNode(7161.000, -0.121 | dict_keys([Observation((0, 1, 0, 1)), Observation((0, 1, 1, 0))]))} State((2, 2, 1, 1, 0, 0), 0, 1) Observation((1, 1, 0, 0))\n",
      "{Action(0): 0.6094659566879272, Action(1): -0.12121235579252243}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 2, 1, 1, 0, 0), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 0, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "VNode(270518.000, -0.192 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(140013.000, -0.192 | dict_keys([Observation((2, 1, 0, 0)), Observation((1, 2, 0, 0))])), Action(1): QNode(130506.000, -0.199 | dict_keys([Observation((1, 1, 1, 0)), Observation((1, 1, 0, 1))]))} State((2, 3, 2, 2, 0, 0), 1, 1) Observation((2, 1, 0, 0))\n",
      "{Action(0): -0.191839337348938, Action(1): -0.19860392808914185}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 3, 2, 2, 0, 0), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 2, 0, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 4, 2, 2, 0, 0), $, 0), 4 1\n",
      "VNode(28715.000, -2.576 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14360.000, -2.576 | dict_keys([Observation((2, 2, 0, 0))])), Action(1): QNode(14356.000, -2.576 | dict_keys([Observation((2, 1, 1, 0))]))} State((2, 4, 2, 2, 0, 0), $, 0) Observation((2, 2, 0, 0))\n",
      "{Action(0): -2.5758354663848877, Action(1): -2.5759246349334717}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 2.0\n",
      "VNode(12632.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "average reward  1.8857142857142857 *** iter = 70\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_problems/cardsample/cardsample_problem.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, next_state, action)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_mismatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mall\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \"\"\"\n\u001b[0;32m-> 2398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2a6b52f50a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                            num_visits_init=1)\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmcp_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmc_average\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard_problem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpomcp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rewards_pomcp%d.npy\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_belief_part\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_problems/cardsample/cardsample_problem.py\u001b[0m in \u001b[0;36mmc_average\u001b[0;34m(card_problem, planner, n_iter, file_name, init_b, reuse, T)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mcard_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_belief\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_problems/cardsample/cardsample_problem.py\u001b[0m in \u001b[0;36mepisode_planner\u001b[0;34m(card_problem, planner, nsteps, reuse)\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==== Step %d ====\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mif\u001b[0m  \u001b[0mreuse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcard_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcard_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/algorithms/pomcp.pyx\u001b[0m in \u001b[0;36mpomdp_py.algorithms.pomcp.POMCP.plan\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/algorithms/po_uct.pyx\u001b[0m in \u001b[0;36mpomdp_py.algorithms.po_uct.POUCT.plan\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/algorithms/po_uct.pyx\u001b[0m in \u001b[0;36mpomdp_py.algorithms.po_uct.POUCT._search\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/algorithms/pomcp.pyx\u001b[0m in \u001b[0;36mpomdp_py.algorithms.pomcp.POMCP._simulate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/algorithms/po_uct.pyx\u001b[0m in \u001b[0;36mpomdp_py.algorithms.po_uct.POUCT._simulate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/algorithms/pomcp.pyx\u001b[0m in \u001b[0;36mpomdp_py.algorithms.pomcp.POMCP._simulate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/algorithms/po_uct.pyx\u001b[0m in \u001b[0;36mpomdp_py.algorithms.po_uct.POUCT._simulate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/framework/basics.pyx\u001b[0m in \u001b[0;36mpomdp_py.framework.basics.sample_generative_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_py/framework/basics.pyx\u001b[0m in \u001b[0;36mpomdp_py.framework.basics.sample_explict_models\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Music/21winter/approx_algo/pomdp-py/pomdp_problems/cardsample/cardsample_problem.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, next_state, action)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_mismatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_mismatches\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_a\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prob false sp_a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m#print(next_state.val, next_state.card, sp_a, action, a_mismatches)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "T = P.m*P.n\n",
    "init_true_state= get_random_state()\n",
    "init_belief_hist = init_belief_particle_s0(all_states)\n",
    "init_belief_part = init_belief_particle_s0(all_states)\n",
    "card_problem = CardProblem(init_true_state, init_belief_hist)\n",
    "card_problem.agent.tree = None\n",
    "\n",
    "print(\"*** Testing POMCP ***\")\n",
    "\n",
    "card_problem.agent.tree = None\n",
    "n_iter = 100\n",
    "reuse = False\n",
    "\n",
    "pomcp = pomdp_py.POMCP(max_depth=T//2, discount_factor=1.,\n",
    "                           num_sims=20000, exploration_const=20,\n",
    "                           rollout_policy=card_problem.agent.policy_model,\n",
    "                           num_visits_init=1)\n",
    "\n",
    "mcp_rewards = mc_average(card_problem, pomcp, n_iter, \"rewards_pomcp%d.npy\"%n_iter, init_belief_part, reuse, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0,1,2,3,4,5,6,7,8,9'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(str(i) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(P.n):\n",
    "    print(card_problem.agent.tree[a], card_problem.agent.tree)\n",
    "    print(tree_i[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = pomcp\n",
    "\n",
    "s0 = env_reset_s0(card_problem)\n",
    "print(\"s0 \",   s0)\n",
    "total_reward = 0\n",
    "action = planner.plan(card_problem.agent)\n",
    "#action = policy.sample(card_problem.agent.cur_belief)\n",
    "\n",
    "true_state = copy.deepcopy(card_problem.env.state)\n",
    "env_reward = card_problem.env.state_transition(action, execute=True)\n",
    "true_next_state = copy.deepcopy(card_problem.env.state)\n",
    "real_observation = card_problem.env.provide_observation(card_problem.agent.observation_model, action)\n",
    "card_problem.agent.update_history(action, real_observation)    \n",
    "    \n",
    "    \n",
    "print(\"True state: %s, %d %d\" % (true_state, np.array(true_state.val)[::3].sum(), int(true_state.terminal)))\n",
    "print(\"Action: %s\" % str(action))\n",
    "print(\"Observation: %s,  %d\" % (str(real_observation), np.array(real_observation.val).sum()))\n",
    "print(\"Reward: %s\" % str(np.maximum(0, env_reward)))\n",
    "print(\"True next state: %s, %d %d\" % (true_next_state, \n",
    "    np.array(true_next_state.val)[::3].sum(), int(true_next_state.terminal)))\n",
    "\n",
    "env_reward = card_problem.env.state_transition(action, execute=True)\n",
    "true_next_state = copy.deepcopy(card_problem.env.state)\n",
    "real_observation = card_problem.env.provide_observation(card_problem.agent.observation_model, action)\n",
    "card_problem.agent.update_history(action, real_observation)    \n",
    "    \n",
    "    \n",
    "print(\"True state: %s, %d %d\" % (true_state, np.array(true_state.val)[::3].sum(), int(true_state.terminal)))\n",
    "print(\"Action: %s\" % str(action))\n",
    "print(\"Observation: %s,  %d\" % (str(real_observation), np.array(real_observation.val).sum()))\n",
    "print(\"Reward: %s\" % str(np.maximum(0, env_reward)))\n",
    "print(\"True next state: %s, %d %d\" % (true_next_state, \n",
    "    np.array(true_next_state.val)[::3].sum(), int(true_next_state.terminal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s0 = State(tuple(np.zeros(3*P.n, np.int32)), -1)\n",
    "all_states = set()\n",
    "create_all_states(s0, all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P.n*2)**(P.n*P.m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states for n=5;m=2 is 330438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in all_states:\n",
    "    val = np.array(s.val)\n",
    "    if (val[::3].sum()-1 != val[1::3].sum()) and s.card != \"$\" :\n",
    "        print(s, val[::3].sum(), val[1::3].sum(),\"error in states actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs = create_all_observations(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all = np.load(\"rewards_pomcp100.npy\")\n",
    "\n",
    "num = (rewards_all != -2*T).sum()\n",
    "print(num)\n",
    "\n",
    "print(rewards_all.sum() * 1.0 / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all = np.load(\"rewards_pouct500_colab.npy\")\n",
    "\n",
    "num = (rewards_all != -2*T).sum()\n",
    "print(num)\n",
    "\n",
    "print(rewards_all.sum() * 1.0 / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_problem.agent.tree.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from julia.api import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "\n",
    "from julia.QMDP import QMDPSolver\n",
    "from julia.SARSOP import SARSOPSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
