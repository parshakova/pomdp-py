{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|S| = 45\n",
      "|O| = 35\n"
     ]
    }
   ],
   "source": [
    "from cardsample_problem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Testing POMCP ***\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 0, 1, 0), 0, 0), 2 0\n",
      "VNode(40000.000, 0.076 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(19853.000, 0.042 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(20148.000, 0.076 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((2, 0, 0, 0, 1, 0), 0, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.04170655086636543, Action(1): 0.07569003850221634}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 0, 0, 0, 1, 0), 0, 0), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 2)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 1, 2, 0), 1, 0), 3 0\n",
      "VNode(50123.000, -0.396 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(21138.000, -1.057 | dict_keys([Observation((0, 1, 0, 1)), Observation((1, 0, 0, 1))])), Action(1): QNode(28986.000, -0.396 | dict_keys([Observation((0, 0, 1, 1)), Observation((0, 0, 0, 2))]))} State((2, 0, 0, 1, 2, 0), 1, 0) Observation((0, 0, 0, 2))\n",
      "{Action(0): -1.0568090677261353, Action(1): -0.39594945311546326}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 0, 0, 1, 2, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 2)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 0, 0, 2, 3, 1), 1, 1), 4 0\n",
      "VNode(49536.000, -0.805 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(14753.000, -2.693 | dict_keys([Observation((0, 1, 0, 2))])), Action(1): QNode(34784.000, -0.805 | dict_keys([Observation((0, 0, 1, 2))]))} State((2, 0, 0, 2, 3, 1), 1, 1) Observation((0, 0, 1, 2))\n",
      "{Action(0): -2.693079710006714, Action(1): -0.8050245642662048}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 0, 0, 2, 3, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 1, 2)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 0, 2, 3, 1), $, 0), 4 1\n",
      "VNode(73805.000, -2.476 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(36903.000, -2.476 | dict_keys([Observation((0, 1, 1, 2))])), Action(1): QNode(36903.000, -2.477 | dict_keys([Observation((0, 0, 2, 2))]))} State((2, 1, 0, 2, 3, 1), $, 0) Observation((0, 1, 1, 2))\n",
      "{Action(0): -2.4764962196350098, Action(1): -2.476525068283081}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(34491.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 1\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(80000.000, 0.089 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(39751.000, 0.068 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(40250.000, 0.089 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.06757041811943054, Action(1): 0.08852178603410721}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(60105.000, -0.272 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(34970.000, -0.272 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(25136.000, -0.909 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.2716892659664154, Action(1): -0.9086149334907532}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "VNode(63382.000, -1.515 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(31726.000, -1.515 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(31657.000, -1.519 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 2, 1, 2, 1, 1), 0, 0) Observation((1, 1, 1, 0))\n",
      "{Action(0): -1.5151876211166382, Action(1): -1.5192413330078125}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(54057.000, -2.665 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27029.000, -2.665 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27029.000, -2.665 | dict_keys([Observation((1, 1, 1, 1))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.664987325668335, Action(1): -2.664987325668335}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(25007.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 2\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 0, 1, 0), 0, 0), 2 0\n",
      "VNode(120000.000, 0.105 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(59599.000, 0.087 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(60402.000, 0.105 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((2, 0, 0, 0, 1, 0), 0, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.08654483407735825, Action(1): 0.105277881026268}\n",
      "==== Step 2 ====\n",
      "True state: State((2, 0, 0, 0, 1, 0), 0, 0), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 2)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 1, 2, 0), 1, 0), 3 0\n",
      "VNode(70151.000, -0.171 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(28873.000, -0.815 | dict_keys([Observation((0, 1, 0, 1)), Observation((1, 0, 0, 1))])), Action(1): QNode(41279.000, -0.171 | dict_keys([Observation((0, 0, 1, 1)), Observation((0, 0, 0, 2))]))} State((2, 0, 0, 1, 2, 0), 1, 0) Observation((0, 0, 0, 2))\n",
      "{Action(0): -0.8147059679031372, Action(1): -0.1711529791355133}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 0, 0, 1, 2, 0), 1, 0), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 2)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 0, 0, 2, 3, 1), 1, 1), 4 0\n",
      "VNode(53777.000, -0.684 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(15899.000, -2.528 | dict_keys([Observation((0, 1, 0, 2))])), Action(1): QNode(37879.000, -0.684 | dict_keys([Observation((0, 0, 1, 2))]))} State((2, 0, 0, 2, 3, 1), 1, 1) Observation((0, 0, 1, 2))\n",
      "{Action(0): -2.5277066230773926, Action(1): -0.6842036843299866}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 0, 0, 2, 3, 1), 1, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 2, 2)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 2, 4, 2), $, 0), 4 1\n",
      "VNode(74530.000, -2.466 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(37265.000, -2.466 | dict_keys([Observation((0, 1, 1, 2))])), Action(1): QNode(37266.000, -2.466 | dict_keys([Observation((0, 0, 2, 2))]))} State((2, 0, 0, 2, 4, 2), $, 0) Observation((0, 0, 2, 2))\n",
      "{Action(0): -2.4659829139709473, Action(1): -2.4659438133239746}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(34634.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 3\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "VNode(160000.000, 0.121 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(79502.000, 0.105 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(80499.000, 0.121 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 0), 1, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.10544396191835403, Action(1): 0.12072201073169708}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 1)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "VNode(80131.000, -0.086 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(32530.000, -0.732 | dict_keys([Observation((0, 1, 0, 1)), Observation((1, 0, 0, 1))])), Action(1): QNode(47602.000, -0.086 | dict_keys([Observation((0, 0, 1, 1)), Observation((0, 0, 0, 2))]))} State((1, 0, 0, 2, 2, 1), 1, 1) Observation((0, 0, 1, 1))\n",
      "{Action(0): -0.7324010133743286, Action(1): -0.0864887610077858}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 0, 2, 2, 1), 0, 0), 4 0\n",
      "VNode(71882.000, -1.338 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(35963.000, -1.338 | dict_keys([Observation((0, 1, 1, 1)), Observation((1, 0, 1, 1))])), Action(1): QNode(35920.000, -1.340 | dict_keys([Observation((0, 0, 2, 1)), Observation((0, 0, 1, 2))]))} State((2, 1, 0, 2, 2, 1), 0, 0) Observation((0, 1, 1, 1))\n",
      "{Action(0): -1.33840012550354, Action(1): -1.340429663658142}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 0, 2, 2, 1), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(54063.000, -2.665 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27032.000, -2.665 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(27032.000, -2.665 | dict_keys([Observation((0, 1, 1, 2))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.6645421981811523, Action(1): -2.664579153060913}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(25000.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 4\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(200000.000, 0.132 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(99336.000, 0.118 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(100665.000, 0.132 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.11757081747055054, Action(1): 0.13224011659622192}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(90423.000, -0.034 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(53834.000, -0.034 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(36590.000, -0.654 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): -0.034011173993349075, Action(1): -0.6541972756385803}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "VNode(75752.000, -1.261 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(37714.000, -1.276 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(38039.000, -1.261 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 1, 1, 2, 2, 1), 1, 0) Observation((1, 0, 1, 1))\n",
      "{Action(0): -1.2755411863327026, Action(1): -1.260682225227356}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 1, 2, 2, 1), 1, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 0, 2, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 1, 2, 3, 2), $, 0), 4 1\n",
      "VNode(53852.000, -2.668 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(26926.000, -2.668 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(26927.000, -2.668 | dict_keys([Observation((1, 0, 2, 1))]))} State((2, 1, 1, 2, 3, 2), $, 0) Observation((1, 0, 2, 1))\n",
      "{Action(0): -2.6680519580841064, Action(1): -2.668027400970459}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(24919.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 5\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(240000.000, 0.141 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(119401.000, 0.131 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(120600.000, 0.141 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.13100405037403107, Action(1): 0.14115142822265625}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(100272.000, 0.023 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(60368.000, 0.023 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(39905.000, -0.612 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): 0.023289142176508904, Action(1): -0.61195307970047}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(80398.000, -1.195 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(40310.000, -1.195 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(40089.000, -1.204 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -1.1951868534088135, Action(1): -1.2043473720550537}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(54235.000, -2.660 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27118.000, -2.660 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27118.000, -2.660 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.659782886505127, Action(1): -2.659782886505127}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(25014.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 6\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "VNode(280000.000, 0.148 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(139514.000, 0.142 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(140487.000, 0.148 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 0), 1, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.1416410505771637, Action(1): 0.14819788932800293}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 1)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 0, 0, 1, 2, 1), 0, 1), 3 0\n",
      "VNode(110173.000, 0.061 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(43152.000, -0.587 | dict_keys([Observation((0, 1, 0, 1)), Observation((1, 0, 0, 1))])), Action(1): QNode(67022.000, 0.061 | dict_keys([Observation((0, 0, 1, 1)), Observation((0, 0, 0, 2))]))} State((2, 0, 0, 1, 2, 1), 0, 1) Observation((0, 0, 1, 1))\n",
      "{Action(0): -0.587017834186554, Action(1): 0.06121916323900223}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 0, 0, 1, 2, 1), 0, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 2)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 2, 3, 1), 1, 0), 4 0\n",
      "VNode(84881.000, -1.135 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(42373.000, -1.140 | dict_keys([Observation((0, 1, 1, 1)), Observation((1, 0, 1, 1))])), Action(1): QNode(42509.000, -1.135 | dict_keys([Observation((0, 0, 2, 1)), Observation((0, 0, 1, 2))]))} State((2, 0, 0, 2, 3, 1), 1, 0) Observation((0, 0, 1, 2))\n",
      "{Action(0): -1.1401174068450928, Action(1): -1.1349095106124878}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 0, 0, 2, 3, 1), 1, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 1, 2)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 0, 2, 3, 1), $, 0), 4 1\n",
      "VNode(54134.000, -2.660 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27068.000, -2.660 | dict_keys([Observation((0, 1, 1, 2))])), Action(1): QNode(27067.000, -2.660 | dict_keys([Observation((0, 0, 2, 2))]))} State((2, 1, 0, 2, 3, 1), $, 0) Observation((0, 1, 1, 2))\n",
      "{Action(0): -2.6603732109069824, Action(1): -2.6604347229003906}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 1.0\n",
      "VNode(24947.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 7\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(320000.000, 0.159 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(159062.000, 0.149 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(160939.000, 0.159 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.14852599799633026, Action(1): 0.15898005664348602}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(120586.000, 0.097 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(73671.000, 0.097 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(46916.000, -0.541 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): 0.09743200242519379, Action(1): -0.5405393242835999}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "VNode(89266.000, -1.087 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(44635.000, -1.087 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(44632.000, -1.087 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 2, 1, 2, 1, 1), 0, 0) Observation((1, 1, 1, 0))\n",
      "{Action(0): -1.0868799686431885, Action(1): -1.0869982242584229}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(54472.000, -2.654 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27237.000, -2.654 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27236.000, -2.654 | dict_keys([Observation((1, 1, 1, 1))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.654402017593384, Action(1): -2.654426097869873}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(25065.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 8\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "VNode(360000.000, 0.167 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(179068.000, 0.158 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(180933.000, 0.167 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 0), 1, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.15802353620529175, Action(1): 0.16675788164138794}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 1)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "VNode(130315.000, 0.125 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(49721.000, -0.536 | dict_keys([Observation((0, 1, 0, 1)), Observation((1, 0, 0, 1))])), Action(1): QNode(80595.000, 0.125 | dict_keys([Observation((0, 0, 1, 1)), Observation((0, 0, 0, 2))]))} State((1, 0, 0, 2, 2, 1), 1, 1) Observation((0, 0, 1, 1))\n",
      "{Action(0): -0.5357284545898438, Action(1): 0.12478241324424744}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 1, 1)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 1, 0, 2, 2, 1), 0, 0), 4 0\n",
      "VNode(93910.000, -1.035 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(47036.000, -1.035 | dict_keys([Observation((1, 0, 1, 1)), Observation((0, 1, 1, 1))])), Action(1): QNode(46875.000, -1.040 | dict_keys([Observation((0, 0, 2, 1)), Observation((0, 0, 1, 2))]))} State((2, 1, 0, 2, 2, 1), 0, 0) Observation((0, 1, 1, 1))\n",
      "{Action(0): -1.0346730947494507, Action(1): -1.0400426387786865}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 1, 0, 2, 2, 1), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(54329.000, -2.656 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27165.000, -2.656 | dict_keys([Observation((1, 1, 1, 1))])), Action(1): QNode(27165.000, -2.656 | dict_keys([Observation((0, 1, 1, 2))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.6558759212493896, Action(1): -2.6559157371520996}\n",
      "Reward (Cumulative): 1.0\n",
      "VNode(24986.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 9\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((0, 0, 0, 2, 1, 1), 1, 1), 2 0\n",
      "VNode(400000.000, 0.176 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(198527.000, 0.164 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(201474.000, 0.176 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((0, 0, 0, 2, 1, 1), 1, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.16402661800384521, Action(1): 0.1758785992860794}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 0, 0, 2, 1, 1), 1, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 1, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 2, 1, 1), 0, 0), 3 0\n",
      "VNode(140983.000, 0.152 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(87353.000, 0.152 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(53631.000, -0.491 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((1, 1, 0, 2, 1, 1), 0, 0) Observation((0, 1, 1, 0))\n",
      "{Action(0): 0.15235869586467743, Action(1): -0.4913017451763153}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 0, 2, 1, 1), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 2, 1, 1), 0, 1), 4 0\n",
      "VNode(69071.000, -0.342 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(49664.000, -0.342 | dict_keys([Observation((1, 1, 1, 0))])), Action(1): QNode(19408.000, -2.139 | dict_keys([Observation((0, 1, 1, 1))]))} State((2, 2, 1, 2, 1, 1), 0, 1) Observation((1, 1, 1, 0))\n",
      "{Action(0): -0.34196171164512634, Action(1): -2.1385414600372314}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 1, 2, 1, 1), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(76439.000, -2.442 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(38220.000, -2.442 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(38220.000, -2.442 | dict_keys([Observation((1, 1, 1, 1))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.441650152206421, Action(1): -2.441678047180176}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(35104.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 10\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(440000.000, 0.181 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(218580.000, 0.171 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(221421.000, 0.181 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.17101632058620453, Action(1): 0.18095816671848297}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(150939.000, 0.176 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(94100.000, 0.176 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(56840.000, -0.470 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): 0.17550237476825714, Action(1): -0.46996888518333435}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "VNode(102824.000, -0.935 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(51635.000, -0.935 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(51190.000, -0.948 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 1, 2, 1, 1), 0, 0) Observation((1, 1, 1, 0))\n",
      "{Action(0): -0.934846043586731, Action(1): -0.9478003978729248}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(54318.000, -2.656 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27160.000, -2.656 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27159.000, -2.656 | dict_keys([Observation((1, 1, 1, 1))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.655740261077881, Action(1): -2.655764579772949}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(24974.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 11\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(480000.000, 0.187 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(238523.000, 0.178 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(241478.000, 0.187 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.1779685616493225, Action(1): 0.1870766431093216}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(161016.000, 0.193 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(101065.000, 0.193 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(59952.000, -0.457 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): 0.1929800808429718, Action(1): -0.45693257451057434}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(107429.000, -0.901 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(54043.000, -0.901 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(53387.000, -0.919 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -0.9014179110527039, Action(1): -0.9192895889282227}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(54517.000, -2.653 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27259.000, -2.653 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27259.000, -2.653 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.653216600418091, Action(1): -2.6532516479492188}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(25069.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 12\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(520000.000, 0.194 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(258049.000, 0.184 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(261952.000, 0.194 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.18354596197605133, Action(1): 0.1942315250635147}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(171286.000, 0.214 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(108087.000, 0.214 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(63200.000, -0.436 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): 0.2137925624847412, Action(1): -0.4361548125743866}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(112101.000, -0.864 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(56237.000, -0.864 | dict_keys([Observation((1, 1, 1, 0)), Observation((2, 0, 1, 0))])), Action(1): QNode(55865.000, -0.874 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -0.8639524579048157, Action(1): -0.8735148906707764}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(54404.000, -2.654 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27203.000, -2.654 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27202.000, -2.654 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.654153347015381, Action(1): -2.6542153358459473}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(25002.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 13\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((0, 0, 0, 2, 1, 1), 1, 1), 2 0\n",
      "VNode(560000.000, 0.199 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(278129.000, 0.190 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(281872.000, 0.199 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((0, 0, 0, 2, 1, 1), 1, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.1900552213191986, Action(1): 0.19925139844417572}\n",
      "==== Step 2 ====\n",
      "True state: State((0, 0, 0, 2, 1, 1), 1, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((0, 1, 1, 0)),  2\n",
      "Reward: 0.0\n",
      "True next state: State((1, 1, 0, 2, 1, 1), 0, 0), 3 0\n",
      "VNode(181218.000, 0.230 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(115025.000, 0.230 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(66194.000, -0.423 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((1, 1, 0, 2, 1, 1), 0, 0) Observation((0, 1, 1, 0))\n",
      "{Action(0): 0.23007123172283173, Action(1): -0.42289426922798157}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 0, 2, 1, 1), 0, 0), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 1, 2, 1, 1), 0, 1), 4 0\n",
      "VNode(78269.000, -0.192 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(57040.000, -0.192 | dict_keys([Observation((1, 1, 1, 0))])), Action(1): QNode(21230.000, -1.988 | dict_keys([Observation((0, 1, 1, 1))]))} State((2, 2, 1, 2, 1, 1), 0, 1) Observation((1, 1, 1, 0))\n",
      "{Action(0): -0.19165962934494019, Action(1): -1.9882900714874268}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 1, 2, 1, 1), 0, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(77281.000, -2.432 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(38641.000, -2.432 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(38641.000, -2.432 | dict_keys([Observation((1, 1, 1, 1))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.4321846961975098, Action(1): -2.4322104454040527}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward (Cumulative): 2.0\n",
      "VNode(35345.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 14\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(600000.000, 0.204 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(298325.000, 0.196 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(301676.000, 0.204 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.19638215005397797, Action(1): 0.20381952822208405}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "VNode(191122.000, 0.244 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(121985.000, 0.244 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(69138.000, -0.412 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((2, 1, 1, 1, 1, 1), 0, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): 0.24373574554920197, Action(1): -0.4118594527244568}\n",
      "==== Step 3 ====\n",
      "True state: State((2, 1, 1, 1, 1, 1), 0, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 0, 1, 0)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "VNode(121312.000, -0.797 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(60894.000, -0.797 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(60419.000, -0.808 | dict_keys([Observation((1, 0, 1, 1)), Observation((1, 0, 2, 0))]))} State((2, 2, 2, 2, 1, 1), 1, 1) Observation((2, 0, 1, 0))\n",
      "{Action(0): -0.7972174882888794, Action(1): -0.8081238865852356}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 2, 2, 1, 1), 1, 1), 4 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((2, 1, 1, 0)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 3, 2, 2, 1, 1), $, 0), 4 1\n",
      "VNode(54594.000, -2.651 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27298.000, -2.651 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27297.000, -2.651 | dict_keys([Observation((2, 0, 2, 0))]))} State((2, 3, 2, 2, 1, 1), $, 0) Observation((2, 1, 1, 0))\n",
      "{Action(0): -2.650669574737549, Action(1): -2.6506941318511963}\n",
      "Reward (Cumulative): 3.0\n",
      "VNode(25064.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 15\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "VNode(640000.000, 0.210 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(317904.000, 0.202 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(322097.000, 0.210 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 0), 1, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.2019696831703186, Action(1): 0.21044288575649261}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 1)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "VNode(200651.000, 0.259 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(71374.000, -0.413 | dict_keys([Observation((0, 1, 0, 1)), Observation((1, 0, 0, 1))])), Action(1): QNode(129278.000, 0.259 | dict_keys([Observation((0, 0, 1, 1)), Observation((0, 0, 0, 2))]))} State((1, 0, 0, 2, 2, 1), 1, 1) Observation((0, 0, 1, 1))\n",
      "{Action(0): -0.41337141394615173, Action(1): 0.2587828040122986}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 2, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 0, 0, 2, 3, 2), 0, 1), 4 0\n",
      "VNode(126319.000, -0.773 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(63096.000, -0.776 | dict_keys([Observation((1, 0, 1, 1)), Observation((0, 1, 1, 1))])), Action(1): QNode(63224.000, -0.773 | dict_keys([Observation((0, 0, 2, 1)), Observation((0, 0, 1, 2))]))} State((2, 0, 0, 2, 3, 2), 0, 1) Observation((0, 0, 2, 1))\n",
      "{Action(0): -0.775895357131958, Action(1): -0.7731512784957886}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 0, 0, 2, 3, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 2, 2)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 2, 4, 2), $, 0), 4 1\n",
      "VNode(54604.000, -2.650 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27302.000, -2.650 | dict_keys([Observation((1, 0, 2, 1))])), Action(1): QNode(27303.000, -2.650 | dict_keys([Observation((0, 0, 2, 2))]))} State((2, 0, 0, 2, 4, 2), $, 0) Observation((0, 0, 2, 2))\n",
      "{Action(0): -2.6499505043029785, Action(1): -2.64992618560791}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(25052.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 16\n",
      "s0  State((0, 0, 0, 1, 0, 0), 1, None)\n",
      "==== Step 1 ====\n",
      "True state: State((0, 0, 0, 1, 0, 0), 1, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 0)),  1\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "VNode(680000.000, 0.216 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(337733.000, 0.208 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(342268.000, 0.216 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 1), 0, 1) Observation((0, 0, 1, 0))\n",
      "{Action(0): 0.20777960121631622, Action(1): 0.2161683887243271}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 1), 0, 1), 2 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 0, 1, 0)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "VNode(211678.000, 0.272 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(136780.000, 0.272 | dict_keys([Observation((1, 0, 1, 0)), Observation((0, 1, 1, 0))])), Action(1): QNode(74899.000, -0.394 | dict_keys([Observation((0, 0, 2, 0)), Observation((0, 0, 1, 1))]))} State((1, 1, 1, 2, 1, 1), 1, 1) Observation((1, 0, 1, 0))\n",
      "{Action(0): 0.2715238928794861, Action(1): -0.39387795329093933}\n",
      "==== Step 3 ====\n",
      "True state: State((1, 1, 1, 2, 1, 1), 1, 1), 3 0\n",
      "Action: Action(0)\n",
      "Observation: Observation((1, 1, 1, 0)),  3\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "VNode(131247.000, -0.738 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(65890.000, -0.738 | dict_keys([Observation((2, 0, 1, 0)), Observation((1, 1, 1, 0))])), Action(1): QNode(65358.000, -0.749 | dict_keys([Observation((1, 0, 2, 0)), Observation((1, 0, 1, 1))]))} State((2, 2, 1, 2, 1, 1), 0, 0) Observation((1, 1, 1, 0))\n",
      "{Action(0): -0.7377889156341553, Action(1): -0.7486311793327332}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 2, 1, 2, 1, 1), 0, 0), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((1, 1, 1, 1)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 2, 1, 2, 2, 1), $, 0), 4 1\n",
      "VNode(54428.000, -2.652 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27214.000, -2.652 | dict_keys([Observation((2, 1, 1, 0))])), Action(1): QNode(27215.000, -2.652 | dict_keys([Observation((1, 1, 1, 1))]))} State((2, 2, 1, 2, 2, 1), $, 0) Observation((1, 1, 1, 1))\n",
      "{Action(0): -2.6523497104644775, Action(1): -2.6522889137268066}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(24971.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 17\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "VNode(720000.000, 0.221 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(357981.000, 0.214 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(362020.000, 0.221 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 0), 1, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.21383722126483917, Action(1): 0.22071217000484467}\n",
      "==== Step 2 ====\n",
      "True state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 1, 1)),  2\n",
      "Reward: 1.0\n",
      "True next state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "VNode(220464.000, 0.282 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(77058.000, -0.393 | dict_keys([Observation((0, 1, 0, 1)), Observation((1, 0, 0, 1))])), Action(1): QNode(143407.000, 0.282 | dict_keys([Observation((0, 0, 1, 1)), Observation((0, 0, 0, 2))]))} State((1, 0, 0, 2, 2, 1), 1, 1) Observation((0, 0, 1, 1))\n",
      "{Action(0): -0.39262691140174866, Action(1): 0.2820584177970886}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Step 3 ====\n",
      "True state: State((1, 0, 0, 2, 2, 1), 1, 1), 3 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 2, 1)),  3\n",
      "Reward: 1.0\n",
      "True next state: State((2, 0, 0, 2, 3, 2), 0, 1), 4 0\n",
      "VNode(135822.000, -0.722 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(67780.000, -0.727 | dict_keys([Observation((1, 0, 1, 1)), Observation((0, 1, 1, 1))])), Action(1): QNode(68043.000, -0.722 | dict_keys([Observation((0, 0, 2, 1)), Observation((0, 0, 1, 2))]))} State((2, 0, 0, 2, 3, 2), 0, 1) Observation((0, 0, 2, 1))\n",
      "{Action(0): -0.7269096970558167, Action(1): -0.7218239903450012}\n",
      "==== Step 4 ====\n",
      "True state: State((2, 0, 0, 2, 3, 2), 0, 1), 4 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 2, 2)),  4\n",
      "Reward: 0.0\n",
      "True next state: State((2, 0, 0, 2, 4, 2), $, 0), 4 1\n",
      "VNode(54743.000, -2.646 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(27372.000, -2.646 | dict_keys([Observation((1, 0, 2, 1))])), Action(1): QNode(27372.000, -2.646 | dict_keys([Observation((0, 0, 2, 2))]))} State((2, 0, 0, 2, 4, 2), $, 0) Observation((0, 0, 2, 2))\n",
      "{Action(0): -2.6463139057159424, Action(1): -2.6462764739990234}\n",
      "Reward (Cumulative): 2.0\n",
      "VNode(25066.000, 0.000 | dict_keys([Action(0), Action(1)]))\n",
      "*** iter = 18\n",
      "s0  State((1, 0, 0, 0, 0, 0), 0, None)\n",
      "==== Step 1 ====\n",
      "True state: State((1, 0, 0, 0, 0, 0), 0, None), 1 0\n",
      "Action: Action(1)\n",
      "Observation: Observation((0, 0, 0, 1)),  1\n",
      "Reward: 0.0\n",
      "True next state: State((1, 0, 0, 1, 1, 0), 1, 0), 2 0\n",
      "VNode(760000.000, 0.224 | dict_keys([Action(0), Action(1)])) {Action(0): QNode(378146.000, 0.218 | dict_keys([Observation((0, 1, 0, 0)), Observation((1, 0, 0, 0))])), Action(1): QNode(381855.000, 0.224 | dict_keys([Observation((0, 0, 1, 0)), Observation((0, 0, 0, 1))]))} State((1, 0, 0, 1, 1, 0), 1, 0) Observation((0, 0, 0, 1))\n",
      "{Action(0): 0.218208909034729, Action(1): 0.22403086721897125}\n"
     ]
    }
   ],
   "source": [
    "T = P.m*P.n\n",
    "init_true_state= get_random_state()\n",
    "init_belief_hist = init_belief_particle_s0(all_states)\n",
    "init_belief_part = init_belief_particle_s0(all_states)\n",
    "card_problem.agent.tree = None\n",
    "\n",
    "print(\"*** Testing POMCP ***\")\n",
    "\n",
    "card_problem.agent.tree = None\n",
    "n_iter = 100\n",
    "reuse = False\n",
    "\n",
    "pomcp = pomdp_py.POMCP(max_depth=T//2, discount_factor=1.,\n",
    "                           num_sims=40000, exploration_const=200,\n",
    "                           rollout_policy=card_problem.agent.policy_model,\n",
    "                           num_visits_init=1)\n",
    "\n",
    "mcp_rewards = mc_average(card_problem, pomcp, n_iter, \"rewards_pomcp%d.npy\"%n_iter, init_belief_part, reuse, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(P.n):\n",
    "    print(card_problem.agent.tree[a], card_problem.agent.tree)\n",
    "    print(tree_i[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = pomcp\n",
    "\n",
    "s0 = env_reset_s0(card_problem)\n",
    "print(\"s0 \",   s0)\n",
    "total_reward = 0\n",
    "action = planner.plan(card_problem.agent)\n",
    "#action = policy.sample(card_problem.agent.cur_belief)\n",
    "\n",
    "true_state = copy.deepcopy(card_problem.env.state)\n",
    "env_reward = card_problem.env.state_transition(action, execute=True)\n",
    "true_next_state = copy.deepcopy(card_problem.env.state)\n",
    "real_observation = card_problem.env.provide_observation(card_problem.agent.observation_model, action)\n",
    "card_problem.agent.update_history(action, real_observation)    \n",
    "    \n",
    "    \n",
    "print(\"True state: %s, %d %d\" % (true_state, np.array(true_state.val)[::3].sum(), int(true_state.terminal)))\n",
    "print(\"Action: %s\" % str(action))\n",
    "print(\"Observation: %s,  %d\" % (str(real_observation), np.array(real_observation.val).sum()))\n",
    "print(\"Reward: %s\" % str(np.maximum(0, env_reward)))\n",
    "print(\"True next state: %s, %d %d\" % (true_next_state, \n",
    "    np.array(true_next_state.val)[::3].sum(), int(true_next_state.terminal)))\n",
    "\n",
    "env_reward = card_problem.env.state_transition(action, execute=True)\n",
    "true_next_state = copy.deepcopy(card_problem.env.state)\n",
    "real_observation = card_problem.env.provide_observation(card_problem.agent.observation_model, action)\n",
    "card_problem.agent.update_history(action, real_observation)    \n",
    "    \n",
    "    \n",
    "print(\"True state: %s, %d %d\" % (true_state, np.array(true_state.val)[::3].sum(), int(true_state.terminal)))\n",
    "print(\"Action: %s\" % str(action))\n",
    "print(\"Observation: %s,  %d\" % (str(real_observation), np.array(real_observation.val).sum()))\n",
    "print(\"Reward: %s\" % str(np.maximum(0, env_reward)))\n",
    "print(\"True next state: %s, %d %d\" % (true_next_state, \n",
    "    np.array(true_next_state.val)[::3].sum(), int(true_next_state.terminal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s0 = State(tuple(np.zeros(3*P.n, np.int32)), -1)\n",
    "all_states = set()\n",
    "create_all_states(s0, all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(P.n*2)**(P.n*P.m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states for n=5;m=2 is 330438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in all_states:\n",
    "    val = np.array(s.val)\n",
    "    if (val[::3].sum()-1 != val[1::3].sum()) and s.card != \"$\" :\n",
    "        print(s, val[::3].sum(), val[1::3].sum(),\"error in states actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_obs = create_all_observations(all_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all = np.load(\"rewards_pomcp%d.npy\"%n_iter)\n",
    "\n",
    "num = (rewards_all != 0).sum()\n",
    "print(num)\n",
    "\n",
    "print(rewards_all.sum() * 1.0 / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all = np.load(\"rewards_pouct500_colab.npy\")\n",
    "\n",
    "num = (rewards_all != 0).sum()\n",
    "print(num)\n",
    "\n",
    "print(rewards_all.sum() * 1.0 / num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_problem.agent.tree.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from julia.api import Julia\n",
    "jl = Julia(compiled_modules=False)\n",
    "\n",
    "from julia.QMDP import QMDPSolver\n",
    "from julia.SARSOP import SARSOPSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
